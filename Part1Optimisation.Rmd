---
title: "Part 1 optimisation in class"
author: "Willem Vervoort"
date: "2 July 2018"
output: pdf_document
---

This is the document that gives the code that was done in class for the first part of day 1

```{r setup, echo = F}
# root dir
knitr::opts_knit$set(root.dir = "C:/Users/rver4657/owncloud/Uruguay/coursematerial")
```

Start of real code
You might want to set your working directory

```{r}
setwd("C:/Users/rver4657/owncloud/Uruguay/coursematerial")
```

Load packages
```{r}
require(tidyverse)
```

We are going to generate some random data for the regression. set.seed to make always the same data

```{r}
set.seed(20)

# x data
x <- rnorm(100)

y_fun <- function(x, b0, b1) {
  y_out = b0 + b1*x
  # add random error
  y_out <- y_out + rnorm(length(x),0,0.5)
  return(y_out)
}
```

First have a look at the exact parameters

```{r}
b0 <- 1
b1 <- 1.5
```

Now calculate the y data and make a plot

```{r}
test_data <- data_frame(x = x, # x data in a column called x
y = y_fun(x, b0,b1)) # y data from y_fun
# make a plot
test_data %>%
ggplot(aes(x,y, colour=y, size=y)) + geom_point()
```

exact solution using lm()
This is simply linear regression

```{r}
mod <- lm(y~x,test_data)
summary(mod)
```

Check the assumptions for linear regression in R

```{r}
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
```
# Monte Carlo
Example of Monte Carlo optimisation

```{r}
# uniform samples between min and maximum values
# range of parameters
b0_range <- runif(200,0,2)
b1_range <- runif(200,0.5,3)
# define a storage data frame for the results of the objective functions
Result <- data.frame(bias = rep(0,length(b0_range)),
    RMSE = rep(0,length(b0_range)))

# run a loop of 200 (for the 200 samples)
for (i in 1:length(b0_range)) {
  # calculate the y values
  y_pred <- y_fun(b0_range[i], b1_range[i],test_data$x)
  # Objective functions
  # calculare the bias between observed and predicted
  Result$bias[i] <- mean(y_pred - test_data$y)
  # calculate the rmse for the model
  Result$RMSE[i] <- sqrt(mean((y_pred -
                                 test_data$y)^2))
}

# plot results
Result %>%
  mutate(b0 = b0_range, b1 = b1_range) %>%
  gather(key = "performance", 
         value = "value_perf", bias:RMSE) %>%
  gather(key = "parameter", 
         value = "value_par", b0,b1) %>%
  ggplot(aes(value_par,value_perf, colour=parameter)) +
  geom_point() + 
  facet_wrap(~performance, scales = "free") +
  xlab("Parameter value") + ylab("Performance value")

```

Monte Carlo not optimal, as you might need a lot of samples to clearly define the minimum objective function. Also using Latin hypercube sampling might be more useful as you guarantee sampling each part of the distribution.

# Using optim

"Real optimisation": hill climbing

First define a function for the objective function, in this case the Sum Squared Error

```{r}
# define the sum squared of error as optimisation goal
obj_fun <- function(par,x,data) {
  # par is a vector of parameters b0 and b1
  # x is a vector of input x data
  # data is the observed data to fit
  resid <- data-y_fun(par[1],par[2],x)
  # penalised least squares
  SSE <- sum((resid)^2)
  return(SSE)
}
```

Now can use optim

```{r}
# define some initial guesses for b0 and b1
par_in <- c(0.9,2.5)
fit <- optim(par_in, obj_fun, 
             x = test_data$x,
             data = test_data$y)
# inspect what is in fit
str(fit)
```

Extract different elements
```{r}
fit$par
```

## penalised SSE

```{r}
# define the sum squared of error as optimisation goal
obj_fun <- function(par,x,data, penalty) {
  # par is a vector of parameters b0 and b1
  # x is a vector of input x data
  # data is the observed data to fit
  # penalty is a penalty applied to the SSE
  resid <- data-y_fun(par[1],par[2],x)
  # penalised least squares
  SSE <- penalty*sum((resid)^2)
  return(SSE)
}
```

Exercise: apply a penalty of 25

```{r}
par_in <- c(0.9,2)
fit <- optim(par_in, obj_fun, 
             x = test_data$x,
             data = test_data$y,
             penalty=25)
# inspect what is in fit
str(fit)

```

### Exercise using r2

Note that you need to maximise r2
```{r}
obj_fun <- function(par,x,data) {
  # par is a vector of parameters b0 and b1
  # x is a vector of input x data
  # data is the observed data to fit
  r2 <- cor(data,y_fun(par[1],par[2],x))^2
  return(-1*r2)
}
```

Optimise

```{r}
par_in <- c(0.9,2)
fit <- optim(par_in, obj_fun, 
             x = test_data$x,
             data = test_data$y)
# inspect what is in fit
str(fit)

```
