---
title: "Monte Carlo and GLUE optimisation"
author: "Willem Vervoort and Rafaël Navas"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
---
```{r setup, echo=F, warning=F, message=F}
# root dir
knitr::opts_knit$set(root.dir = "C:/Users/rver4657/owncloud/Uruguay/coursematerial")
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(lubridate)
require(hydromad)
require(knitr)
```

# Introduction

Packages needed  
```{r, eval = FALSE}
require(tidyverse)
require(lubridate)
require(hydromad)
```

This document demonstrates again the Monte Carlo optimisation, but here we are coupling this to understanding of behavioural and non-behavioural thresholds for optimisation using GLUE.

As we discussed in class, the GLUE approach depends on defining a threshold between "behavioural" and "non-behavioural" models. In other words, we divide them in to "good" and "bad" models. The difficulty is setting the value of the threshold.

However, the nice thing about this is that it allows also showing the confidence intervals (uncertainty) around the prediction. This is because we now have many simulations, and we can use either the objective function value to determine the probability, or just look at the width of the uncertainty

# Setting up the model

To start, we will load the same data as in the demonstration of the distributed model

Load the  `GR4J@distr.R` script: 
```{r}
source("scripts/GR4J@distr.R")
```


## The data  
Load all the data similar to the calibration:  
```{r}
load("data/SL_distdata.RDATA")
```

This will load the following objects in your environment related to the Santa Lucia catchment:  
- *SL_distdata* the zoo data frame with data for all the subbasins;  
- *All_par* a list with GR4J parameters for subbasin 1, 2 and 3;    
- *All_reach.par* a list with the Muskingum parameters for the two reaches connecting the sub basins.  

Select a window of the data, because it is a zoo data frame, we can use `window()` on the data.
```{r define_data_window}
# Use data for 2000 - 2005
Data_in <- window(SL_distdata,
                       start = "2000-01-01",
                       end ="2005-12-31")
```

# Setting up the Monte Carlo simulations

As we did in the simple optimisation examples, we will simply use a uniform dsitribution to sample the data. We will define some maxima and minima for the data to allow a reasonable range to be sampled.

## Building the parameter set to be sampled

This essentially involves sampling each parameters and binding them all together in a data_frame.

```{r sample_pars}
set.seed(20)
# this is for 3 subbasins
for (i in 1:3) {
  x1 <- runif(1000,50,500)
  x2 <- runif(1000,-20,20)
  x3 <- runif(1000,10,200)
  x4 <- runif(1000,0.5,5)
  if (i == 1) {
    parameterSpace <- data_frame(x1,x2,x3,x4)
  } else {
    temp <- data_frame(x1,x2,x3,x4)
    parameterSpace <- cbind(parameterSpace,temp)
  }
}
# for three subbasins
for (i in 1:2) {
  x <- runif(1000,0,0.5)
  k <- runif(1000,1,100)
  routing <- data_frame(k,x)
  parameterSpace <- cbind(parameterSpace,routing)
}

str(parameterSpace)
```

## running the model and storing the objective function values

The next step is similar to the sensitivity analysis. We need to run the model 1000 times (same as the number of rows in the parameterSpace) and calculate the objective function values (we will calculate all four as performance values) and store these in a new data frame. We can then do the sensitivity analysis, and we can identify a possible GLUE cut-off

```{r running_model}
# Storage for the objective functions
Store <- list()
# run a loop
for (i in 1:nrow(parameterSpace)) {
  pars_to_input <- parameterSpace[i,]
  # use the rewrite_pars() function
  input_pars <- rewrite_pars(All_par,All_reach_par,
                             pars_to_input)

  SLrun = GR4JSubBasins.run(sb=3,
                          order = c(1,2,3),
                          Data = Data_in,
                          spar = input_pars$sub,
                          rpar = input_pars$reach,
                          sbnames = c("PasoTroncos",
                                      "FrayMarcos",
                                      "PasoPache"))
  
  # calculate stats using the observed data
  # use plot = F on the plot_results function
  results <- plot_results(SLrun,Data_in, plot=F)
  # calculate stats
  stats <- stats_fun(results, decimal=2)
  # keep track of the run count
  stats$run <- i
  # put into Store
  Store[[i]] <- stats
}

Store_out <- do.call(rbind,Store)

Store_out

```


This has now created another dataframe, which has the results for each station, for the parameter combination that were tested.

We can now look at the relationship between parameters and performance for each of the stations and look at the distribution of performance.

## The distributions of the performance measures

First step is to make a histogram of performance by Subbasin, but ignore rel.bias for now.

```{r performance_dist}
# make a hist
Store_out %>%
  # gather all the performance values
  gather(key = "performance", value = "value",
         r.squared:r.sq.log) %>%
  # make a histogram for each
  ggplot(aes(value, fill = Subbasin)) +
  geom_histogram(alpha=0.5) + facet_wrap(~performance) +
  xlim(c(-2,1))
# wrap by performance measure

```

From this we can see that the distributions are very long tailed. For the r.squared, r.sq.sqrt and r.sq.log, we might choose a performance threshold in the order of 0.5 and still have enough model simulations left. 

Looking at the relationship between r.squared and the parameters is a bit more tricky and requires a bit more data massaging.

First step is to arrange the Store_out dataframe by subbasin and merge it with the parameterSpace, but the Subbasin parameters stacked

```{r arrange}
Store_out_sort <- Store_out %>%
  arrange(Subbasin)
# note the order, as the Subbasins are ordered:
# FrayMarcos, PasoPache, PasoTroncos
parameterSpace_stacked <- rbind(parameterSpace[,5:8],
                                parameterSpace[,9:12],
                                parameterSpace[,1:4])

# now combine
Store_out_comb <- cbind(Store_out_sort,parameterSpace_stacked)
```

Now we can gather and plot to show the relationship between the GR4J parameters and the different performance values

```{r}
Store_out_comb %>%
  gather(key="parameter", value="value", x1:x4) %>%
  ggplot(aes(value,r.squared,colour=Subbasin)) +
  geom_point( alpha=0.5) +  facet_wrap(~parameter,
                                       scales="free")
```
That is a bit of a mess and gives little indication of what might the best parameter value. We can do the same for the other performance values, or looking at the routing values. This is useful as it helps understand how parameters behave relative to the objective function. In SWAT-CUP, these are called "dotty plots".

# GLUE

GLUE relies on a choice of a threshold. Based on the distributions of the performance values, we might choose a threshold of 0.5. We could choose other thresholds and this would influence the outcomes.

## Step 1

Set the threshold for GLUE and subset the performance Store_out.
Using a logical combination of all three performance measures, we can select the runs where all three are > 0.5


```{r Threshold}
GLUE_th <- 0.5
Store_out_g <- Store_out %>%
  filter(r.squared > GLUE_th & r.sq.sqrt > GLUE_th &
           r.sq.log > GLUE_th)

```

## step 2

Matching the parameterSpace to the reduced performance measure store. Here we will use the "run" column to identify all the rows in the parameterSpace.

```{r match_pars}
Glue_Store <- cbind(Store_out_g,
                    parameterSpace[Store_out_g$run,])


```

## step 3

Repredict the model output for each of the identified parametersets. Note that this is going to work slightly odd. Basically the results are:  
- if any of the stations has all three performance > 0.5, resimulate the whole basin with the associated parameters.  
- from the results, select only the station that is simulated well (we know this from Store_out_g).   
- combine by station and show variability.  

```{r rerun_models}
# Storage for the model results
FrayMarcos <- NULL
PasoPache <- NULL
PasoTroncos <- NULL
Results <- list("PasoTroncos"=PasoTroncos,
                "FrayMarcos" =FrayMarcos,
                "PasoPache"=PasoPache)

# run a loop
for (i in 1:nrow(Glue_Store)) {
 # i <- 1
  pars_to_input <- Glue_Store[i,7:ncol(Glue_Store)]
  # fix names of parameters
  names(pars_to_input) <- c(rep(c("x1","x2","x3","x4"),3),
                            rep(c("x","k"),2))
  # use the rewrite_pars() function
  input_pars <- rewrite_pars(All_par,All_reach_par,
                             pars_to_input)

  SLrun = GR4JSubBasins.run(sb=3,
                          order = c(1,2,3),
                          Data = Data_in,
                          spar = input_pars$sub,
                          rpar = input_pars$reach,
                          sbnames = c("PasoTroncos",
                                      "FrayMarcos",
                                      "PasoPache"))

  # Select only the station that is well modelled from the output
if (length(Results[[Glue_Store$Subbasin[i]]]) ==0) {
      Results[[Glue_Store$Subbasin[i]]] <-
    SLrun[,colnames(SLrun) %in% Glue_Store$Subbasin[i]]
  } else {
      Results[[Glue_Store$Subbasin[i]]] <-
    merge(Results[[Glue_Store$Subbasin[i]]],
          SLrun[,colnames(SLrun) %in% Glue_Store$Subbasin[i]])
  }
}

```

## step 4

With these results we can plot the cloud of results for each of the stations. We can do this by stepping through the lists, calculating the average across the runs and plotting the resulting data

```{r plotGluePT}
# Paso Los Troncos
AvgResults <- apply(Results[[1]],1,mean)
sdResults <- apply(Results[[1]],1,sd)
plot_df <- as.tibble(cbind(AvgResults,
                           (AvgResults - 2*sdResults),
                           (AvgResults + 2*sdResults)))
names(plot_df) <- c("mean","neg_95ci", "pos_95ci")
plot_df <- plot_df %>%
  mutate(fecha = ymd(time(Data_in))) 
plot_df %>%
  gather(key="Glue_results", value="value", mean:pos_95ci) %>%
  ggplot(aes(fecha,value, colour=Glue_results,
             linetype=Glue_results)) + geom_line(size=1.5) +
  ggtitle("PasoLosTroncos") + scale_y_log10()

plot_df %>%
  gather(key="Glue_results", value="value", mean:pos_95ci) %>%
  ggplot(aes(fecha,value, colour=Glue_results,
             linetype=Glue_results)) + geom_line(size=1.5) +
  ggtitle("PasoLosTroncos") + 
  xlim(c(ymd("2002-01-01"),ymd("2003-01-01")))

```

```{r plotGlueFM}
# Fray Marcos
AvgResults <- apply(Results[[2]],1,mean)
sdResults <- apply(Results[[2]],1,sd)
plot_df <- as.tibble(cbind(AvgResults,
                           (AvgResults - 2*sdResults),
                           (AvgResults + 2*sdResults)))
names(plot_df) <- c("mean","neg_95ci", "pos_95ci")
plot_df <- plot_df %>%
  mutate(fecha = ymd(time(Data_in))) 
plot_df %>%
  gather(key="Glue_results", value="value", mean:pos_95ci) %>%
  ggplot(aes(fecha,value, colour=Glue_results,
             linetype=Glue_results)) + geom_line(size=1.5) +
  ggtitle("Fray Marcos") + scale_y_log10()


```

```{r plotGluePP}
# Paso Pache
AvgResults <- apply(Results[[1]],1,mean)
sdResults <- apply(Results[[1]],1,sd)
plot_df <- as.tibble(cbind(AvgResults,
                           (AvgResults - 2*sdResults),
                           (AvgResults + 2*sdResults)))
names(plot_df) <- c("mean","neg_95ci", "pos_95ci")
plot_df <- plot_df %>%
  mutate(fecha = ymd(time(Data_in))) 
plot_df %>%
  gather(key="Glue_results", value="value", mean:pos_95ci) %>%
  ggplot(aes(fecha,value, colour=Glue_results,
             linetype=Glue_results)) + geom_line(size=1.5) +
  ggtitle("Paso Pache") + scale_y_log10()


```
